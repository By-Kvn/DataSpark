# DataSpark

Pipeline ETL moderne utilisant Prefect, MinIO, Spark et Streamlit pour le traitement de donnÃ©es Big Data.

## ğŸ¯ Vue d'ensemble

DataSpark est un pipeline ETL (Extract, Transform, Load) organisÃ© en couches **Bronze**, **Silver** et **Gold** :

- **Bronze** : Ingestion brute des donnÃ©es depuis les sources
- **Silver** : Transformation et nettoyage des donnÃ©es
- **Gold** : AgrÃ©gations et donnÃ©es analytiques prÃªtes Ã  l'emploi

## ğŸ—ï¸ Architecture

- **Prefect** : Orchestration des workflows ETL
- **MinIO** : Stockage objet (S3-compatible) pour les donnÃ©es
- **Apache Spark** : Traitement distribuÃ© des donnÃ©es
- **PostgreSQL** : Base de donnÃ©es pour Prefect
- **Streamlit** : Interface de visualisation

## ğŸš€ DÃ©marrage rapide

Voir le [Guide d'utilisation complet](README_USAGE.md) pour les instructions dÃ©taillÃ©es.

### Installation

```bash
# 1. DÃ©marrer Docker Desktop (macOS)
# 2. DÃ©marrer les services
docker-compose up -d

# 3. Installer les dÃ©pendances Python
chmod +x setup.sh
./setup.sh

# 4. GÃ©nÃ©rer des donnÃ©es de test (optionnel)
python script/generate_data.py

# 5. ExÃ©cuter le pipeline
python flows/orchestration.py
```

## ğŸ“ Structure du projet

```
BigData-ex/
â”œâ”€â”€ flows/              # Flows Prefect (Bronze, Silver, Gold)
â”œâ”€â”€ script/             # Scripts utilitaires
â”œâ”€â”€ data/               # DonnÃ©es sources et gÃ©nÃ©rÃ©es
â”œâ”€â”€ docker-compose.yml  # Configuration Docker
â””â”€â”€ requirements.txt    # DÃ©pendances Python
```

## ğŸ“š Documentation

- [Guide d'utilisation](README_USAGE.md) - Instructions complÃ¨tes
- [Installation](INSTALL.md) - Guide d'installation dÃ©taillÃ©
- [DÃ©pannage](TROUBLESHOOTING.md) - Solutions aux problÃ¨mes courants

## ğŸ› ï¸ Technologies

- Python 3.10+
- Prefect 3.x
- Apache Spark 3.5
- MinIO
- PostgreSQL
- Streamlit
- Pandas, PyArrow

## ğŸ“ License

Ce projet est un exemple Ã©ducatif de pipeline ETL moderne.
