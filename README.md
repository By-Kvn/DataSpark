# DataSpark - Pipeline ETL Moderne avec Architecture Medallion

## ğŸ“‹ PrÃ©sentation du Projet

**DataSpark** est un pipeline ETL (Extract, Transform, Load) moderne implÃ©mentant l'**architecture Medallion** (Bronze-Silver-Gold) pour le traitement de donnÃ©es Big Data. Ce projet dÃ©montre l'utilisation de technologies industrielles pour construire un systÃ¨me de traitement de donnÃ©es scalable, robuste et maintenable.

### ğŸ¯ Objectifs du Projet

1. **ImplÃ©menter une architecture de Data Lake moderne** avec sÃ©paration claire des couches de donnÃ©es
2. **Automatiser le traitement de donnÃ©es** avec orchestration de workflows
3. **Assurer la qualitÃ© des donnÃ©es** Ã  travers des validations et transformations
4. **CrÃ©er des donnÃ©es analytiques prÃªtes Ã  l'emploi** pour la prise de dÃ©cision

---

## ğŸ—ï¸ Architecture du SystÃ¨me

### Architecture Medallion (Bronze-Silver-Gold)

Le projet suit l'architecture **Medallion** popularisÃ©e par Databricks, qui organise les donnÃ©es en trois couches distinctes :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SOURCES (CSV Files)                       â”‚
â”‚              clients.csv, achats.csv                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¥‰ BRONZE LAYER (Raw Data Lake)                            â”‚
â”‚  â€¢ DonnÃ©es brutes, non transformÃ©es                         â”‚
â”‚  â€¢ Format: CSV                                               â”‚
â”‚  â€¢ Bucket MinIO: bronze/                                     â”‚
â”‚  â€¢ Objectif: Archive des donnÃ©es sources                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼ Transformation & Nettoyage
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¥ˆ SILVER LAYER (Cleaned & Validated Data)                  â”‚
â”‚  â€¢ DonnÃ©es nettoyÃ©es et validÃ©es                            â”‚
â”‚  â€¢ Format: Parquet (optimisÃ© pour l'analytique)             â”‚
â”‚  â€¢ Bucket MinIO: silver/                                     â”‚
â”‚  â€¢ Objectif: DonnÃ©es de qualitÃ© pour l'analyse             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼ AgrÃ©gation & Enrichissement
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¥‡ GOLD LAYER (Analytical Data)                            â”‚
â”‚  â€¢ Tables de dimension (star schema)                        â”‚
â”‚  â€¢ Tables de faits                                           â”‚
â”‚  â€¢ KPIs et mÃ©triques                                         â”‚
â”‚  â€¢ AgrÃ©gations temporelles                                  â”‚
â”‚  â€¢ Bucket MinIO: gold/                                       â”‚
â”‚  â€¢ Objectif: DonnÃ©es prÃªtes pour la visualisation/BI        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stack Technologique

| Composant | Technologie | RÃ´le |
|-----------|-------------|------|
| **Orchestration** | Prefect 3.x | Orchestration des workflows ETL avec gestion d'erreurs et retry automatique |
| **Stockage** | MinIO | Stockage objet S3-compatible pour le Data Lake |
| **Base de donnÃ©es** | PostgreSQL | Base de donnÃ©es pour Prefect (mÃ©tadonnÃ©es des workflows) |
| **Traitement** | Apache Spark 3.5 | Traitement distribuÃ© des donnÃ©es (cluster master + 2 workers) |
| **DonnÃ©es** | Pandas + PyArrow | Manipulation et format Parquet |
| **Visualisation** | Streamlit | Interface de visualisation (prÃ©vu) |
| **Conteneurisation** | Docker Compose | Orchestration des services |

---

## ğŸ”„ Fonctionnement du Pipeline

### Ã‰tape 1 : Bronze Layer (Ingestion)

**Objectif** : IngÃ©rer les donnÃ©es brutes depuis les sources vers le Data Lake.

**Processus** :
1. **Upload vers Sources** : Les fichiers CSV (`clients.csv`, `achats.csv`) sont uploadÃ©s dans le bucket `sources` de MinIO
2. **Copie vers Bronze** : Les donnÃ©es sont copiÃ©es dans le bucket `bronze` sans transformation
3. **Format** : CSV (format source prÃ©servÃ©)

**Fichiers traitÃ©s** :
- `clients.csv` : Informations sur les clients (ID, nom, email, date d'inscription, pays)
- `achats.csv` : Transactions d'achats (ID achat, ID client, date, montant, produit)

**RÃ©sultat** : DonnÃ©es brutes archivÃ©es dans MinIO, prÃªtes pour la transformation.

---

### Ã‰tape 2 : Silver Layer (Transformation)

**Objectif** : Nettoyer, valider et transformer les donnÃ©es brutes en donnÃ©es de qualitÃ©.

**Transformations appliquÃ©es** :

1. **Nettoyage des valeurs nulles**
   - DÃ©tection et suppression des lignes avec valeurs nulles critiques
   - Statistiques de nettoyage gÃ©nÃ©rÃ©es

2. **Standardisation des dates**
   - Conversion des formats de dates vers un format standardisÃ©
   - Validation de la cohÃ©rence temporelle

3. **Normalisation des types de donnÃ©es**
   - Conversion des types (int, float, datetime, string)
   - Optimisation de la mÃ©moire

4. **DÃ©duplication**
   - Suppression des enregistrements dupliquÃ©s
   - Conservation des donnÃ©es uniques

5. **ContrÃ´les qualitÃ©**
   - Validation de l'intÃ©gritÃ© rÃ©fÃ©rentielle
   - VÃ©rification des contraintes mÃ©tier
   - GÃ©nÃ©ration de rapports de qualitÃ©

**Format de sortie** : Parquet (format colonnaire optimisÃ© pour l'analytique)

**RÃ©sultats obtenus** :
- **Clients** : 1500 â†’ 1479 lignes (21 lignes avec valeurs nulles supprimÃ©es)
- **Achats** : 23663 â†’ 22422 lignes (1241 lignes avec valeurs nulles supprimÃ©es)
- QualitÃ© validÃ©e : 0 valeurs nulles, 0 doublons, intÃ©gritÃ© rÃ©fÃ©rentielle vÃ©rifiÃ©e

---

### Ã‰tape 3 : Gold Layer (AgrÃ©gation)

**Objectif** : CrÃ©er des donnÃ©es analytiques structurÃ©es pour la Business Intelligence.

**Composants crÃ©Ã©s** :

#### 1. Tables de Dimension (Star Schema)

- **`dim_client`** : Dimension clients enrichie (1479 clients)
- **`dim_produit`** : Dimension produits (10 produits uniques)
- **`dim_temps`** : Dimension temporelle avec attributs (jour, semaine, mois, trimestre, annÃ©e)
- **`dim_pays`** : Dimension gÃ©ographique (9 pays)

#### 2. Table de Faits

- **`fact_achats`** : Table de faits avec toutes les transactions (22422 lignes)
  - ClÃ©s Ã©trangÃ¨res vers les dimensions
  - Mesures : montant, quantitÃ©

#### 3. KPIs et MÃ©triques

- **`kpi_volumes_jour`** : Volume de transactions par jour (346 jours)
- **`kpi_volumes_semaine`** : Volume de transactions par semaine (51 semaines)
- **`kpi_volumes_mois`** : Volume de transactions par mois (12 mois)
- **`kpi_ca_par_pays`** : Chiffre d'affaires par pays (9 pays)
- **`kpi_taux_croissance`** : Taux de croissance mensuel
- **`kpi_distributions_statistiques`** : Statistiques globales (moyenne, mÃ©diane, Ã©cart-type)

#### 4. AgrÃ©gations Temporelles

- **`agregation_jour`** : AgrÃ©gations journaliÃ¨res (CA, nombre de transactions, panier moyen)
- **`agregation_semaine`** : AgrÃ©gations hebdomadaires
- **`agregation_mois`** : AgrÃ©gations mensuelles

**Format de sortie** : Parquet (optimisÃ© pour les requÃªtes analytiques)

---

## ğŸ› ï¸ Ce qui a Ã©tÃ© Mis en Place

### 1. Infrastructure Docker

**Services dÃ©ployÃ©s** :
- **MinIO** : Stockage objet (ports 9000/9001)
  - Buckets crÃ©Ã©s automatiquement : `sources`, `bronze`, `silver`, `gold`
  - Console d'administration accessible
  
- **PostgreSQL** : Base de donnÃ©es pour Prefect (port 5432)
  - Stocke les mÃ©tadonnÃ©es des workflows
  - Historique des exÃ©cutions

- **Prefect Server** : Serveur d'orchestration (port 4200)
  - Interface web pour monitorer les workflows
  - Gestion des tÃ¢ches et retry automatique

- **Apache Spark Cluster** :
  - Master (port 8080) : Interface de monitoring Spark
  - Worker 1 (port 8081) : NÅ“ud de traitement
  - Worker 2 (port 8082) : NÅ“ud de traitement

### 2. Code Python StructurÃ©

**Organisation modulaire** :
```
flows/
â”œâ”€â”€ config.py              # Configuration centralisÃ©e (MinIO, Prefect)
â”œâ”€â”€ bronze_ingestion.py    # Flow Bronze : ingestion des donnÃ©es
â”œâ”€â”€ silver_transformation.py  # Flow Silver : transformation et nettoyage
â”œâ”€â”€ gold_aggregation.py    # Flow Gold : agrÃ©gations et KPIs
â””â”€â”€ orchestration.py       # Orchestration complÃ¨te du pipeline
```

**CaractÃ©ristiques** :
- **Prefect Tasks** : Chaque Ã©tape est une tÃ¢che Prefect avec retry automatique
- **Prefect Flows** : Orchestration des tÃ¢ches avec gestion des dÃ©pendances
- **Gestion d'erreurs** : Retry automatique en cas d'Ã©chec
- **Logging** : TraÃ§abilitÃ© complÃ¨te des opÃ©rations

### 3. Scripts d'Automatisation

- **`setup.sh`** : Installation automatique de l'environnement Python
- **`start-services.sh`** : DÃ©marrage des services Docker
- **`run.sh`** : ExÃ©cution du pipeline complet

### 4. QualitÃ© et Robustesse

- **Validation des donnÃ©es** : ContrÃ´les qualitÃ© Ã  chaque Ã©tape
- **Gestion des erreurs** : Retry automatique sur les opÃ©rations critiques
- **TraÃ§abilitÃ©** : Logs dÃ©taillÃ©s de chaque opÃ©ration
- **ReproductibilitÃ©** : Pipeline idempotent (peut Ãªtre exÃ©cutÃ© plusieurs fois)

---

## ğŸ“Š RÃ©sultats Obtenus

### DonnÃ©es TraitÃ©es

| Couche | Fichiers | Lignes | Format |
|--------|----------|--------|--------|
| **Bronze** | 2 CSV | ~25,000 | CSV |
| **Silver** | 2 Parquet | 23,901 | Parquet |
| **Gold** | 15 Parquet | ~25,000+ | Parquet |

### MÃ©triques de QualitÃ©

- âœ… **0 valeurs nulles** dans les donnÃ©es Silver et Gold
- âœ… **0 doublons** dÃ©tectÃ©s
- âœ… **IntÃ©gritÃ© rÃ©fÃ©rentielle** validÃ©e
- âœ… **1479 clients uniques** traitÃ©s
- âœ… **22422 transactions** analysÃ©es
- âœ… **10 produits** cataloguÃ©s
- âœ… **9 pays** reprÃ©sentÃ©s

### KPIs CalculÃ©s

- Volume de transactions par pÃ©riode (jour/semaine/mois)
- Chiffre d'affaires par pays
- Taux de croissance mensuel
- Statistiques de distribution (moyenne, mÃ©diane, Ã©cart-type)
- AgrÃ©gations temporelles pour l'analyse de tendances

---

## ğŸš€ Utilisation

### PrÃ©requis

- Docker Desktop installÃ© et dÃ©marrÃ©
- Python 3.10+ installÃ©
- Git (pour cloner le projet)

### Installation

```bash
# 1. Cloner le projet
git clone https://github.com/By-Kvn/DataSpark.git
cd DataSpark

# 2. DÃ©marrer les services Docker
docker-compose up -d

# 3. Installer les dÃ©pendances Python
chmod +x setup.sh
./setup.sh

# 4. (Optionnel) Configurer Java pour PySpark
source setup_java.sh

# 5. (Optionnel) GÃ©nÃ©rer des donnÃ©es de test
python script/generate_data.py
```

### ExÃ©cution du Pipeline

```bash
# Activer l'environnement virtuel
source venv/bin/activate

# ExÃ©cuter le pipeline complet
cd flows
python orchestration.py
```

Ou utiliser le script automatique :
```bash
chmod +x run.sh
./run.sh
```

### AccÃ¨s aux Interfaces

- **Prefect UI** : http://localhost:4200
  - Visualiser les workflows
  - Consulter l'historique des exÃ©cutions
  - Monitorer les performances

- **MinIO Console** : http://localhost:9001
  - Login : `minioadmin` / `minioadmin`
  - Explorer les buckets et fichiers
  - TÃ©lÃ©charger les donnÃ©es traitÃ©es

- **Spark Master UI** : http://localhost:8080
  - Monitorer le cluster Spark
  - Voir les applications en cours

---

## ğŸ“ Structure du Projet

```
DataSpark/
â”œâ”€â”€ flows/                              # Code source des flows Prefect
â”‚   â”œâ”€â”€ config.py                      # Configuration centralisÃ©e (MinIO, Prefect, Spark)
â”‚   â”œâ”€â”€ bronze_ingestion.py            # Couche Bronze
â”‚   â”œâ”€â”€ silver_transformation.py       # Couche Silver (Pandas)
â”‚   â”œâ”€â”€ silver_transformation_spark.py # Couche Silver (PySpark)
â”‚   â”œâ”€â”€ gold_aggregation.py            # Couche Gold
â”‚   â””â”€â”€ orchestration.py               # Orchestration complÃ¨te
â”œâ”€â”€ script/
â”‚   â”œâ”€â”€ generate_data.py               # GÃ©nÃ©ration de donnÃ©es de test
â”‚   â””â”€â”€ benchmark_pandas_vs_spark.py   # Script de benchmark
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sources/                       # DonnÃ©es sources CSV
â”œâ”€â”€ docker-compose.yml                 # Configuration Docker (MinIO, Prefect, Spark)
â”œâ”€â”€ requirements.txt                   # DÃ©pendances Python
â”œâ”€â”€ setup.sh                           # Script d'installation
â”œâ”€â”€ start-services.sh                  # DÃ©marrage des services
â”œâ”€â”€ run.sh                             # ExÃ©cution du pipeline
â”œâ”€â”€ README.md                          # Ce fichier
â”œâ”€â”€ README_USAGE.md                    # Guide d'utilisation dÃ©taillÃ©
â”œâ”€â”€ INSTALL.md                         # Guide d'installation
â””â”€â”€ TROUBLESHOOTING.md                 # Guide de dÃ©pannage
```

---

## ğŸ“ Points PÃ©dagogiques

Ce projet dÃ©montre :

1. **Architecture de Data Lake moderne** : ImplÃ©mentation de l'architecture Medallion
2. **Orchestration de workflows** : Utilisation de Prefect pour automatiser les processus ETL
3. **Stockage objet** : Utilisation de MinIO (S3-compatible) pour le Data Lake
4. **QualitÃ© des donnÃ©es** : Validation et nettoyage systÃ©matique
5. **ModÃ©lisation analytique** : CrÃ©ation d'un schÃ©ma en Ã©toile (star schema)
6. **Calcul de KPIs** : AgrÃ©gations et mÃ©triques mÃ©tier
7. **Conteneurisation** : DÃ©ploiement avec Docker Compose
8. **Bonnes pratiques** : Code modulaire, gestion d'erreurs, logging

---

## ğŸ”® AmÃ©liorations Futures

- [x] IntÃ©gration Spark pour le traitement distribuÃ© de grandes volumÃ©tries âœ…
- [x] Script de benchmark Pandas vs PySpark âœ…
- [ ] Version PySpark complÃ¨te pour la couche Gold
- [ ] Interface Streamlit pour visualiser les KPIs
- [ ] Planification automatique (scheduling) avec Prefect
- [ ] Alertes et notifications en cas d'erreur
- [ ] Tests unitaires et d'intÃ©gration
- [ ] Documentation API avec Swagger
- [ ] Pipeline CI/CD avec GitHub Actions

---

## ğŸ“š Technologies UtilisÃ©es

- **Python 3.10+** : Langage de programmation
- **Prefect 3.x** : Orchestration de workflows
- **MinIO** : Stockage objet S3-compatible
- **PostgreSQL** : Base de donnÃ©es relationnelle
- **Apache Spark 3.5** : Traitement distribuÃ©
- **Pandas** : Manipulation de donnÃ©es
- **PyArrow** : Format Parquet
- **Docker & Docker Compose** : Conteneurisation
- **Streamlit** : Interface de visualisation (prÃ©vu)

---

## ğŸ‘¤ Auteur

**Kevin Labatte**
- Projet Ã©ducatif rÃ©alisÃ© dans le cadre d'un cours sur le Big Data
- DÃ©monstration d'un pipeline ETL moderne avec architecture Medallion

---

## ğŸ“ License

Ce projet est un exemple Ã©ducatif de pipeline ETL moderne. Libre d'utilisation pour l'apprentissage.

---

## ğŸ™ Remerciements

Ce projet utilise des technologies open-source et suit les meilleures pratiques de l'industrie pour le traitement de donnÃ©es Big Data.
